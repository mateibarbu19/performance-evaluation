# The security community's take on peer reviewing

Hello, my name is Matthew and today I will be presenting the security
community's take on peer reviewing.

---

---

But first let's define some terms.

---

I would like to ask: How many of you have seen this popular video? It's based on
even popular metascience paper...

---

... depicted on the left.

The science of science, as it were, could be summed up as a birds eye view of
research.

---

Today, we are talking about a quantitative study of science, which rests on two
pillars: peer reviewing and ranking.

---

Peer reviewing is a process that accepts valid and quality articles.

Within the security community it's double blind, in that, both the reviewers and
authors identities are unknown to each other.

---

In this figure we see that every paper is assigned by a Chair (i.e. a organizer)
to 3 or 4 reviewers (who are committee members). Two rounds of reviewing are
done before a discussion makes the call: Accept, Reject or Revise.

---

---

In this sense my paper is a exploratory qualitative study based on interviews
with Program Committee members and Chairs. It's a response to recent contextual
problems like: exponential increase in paper submissions, delegation and rolling
deadlines.

Don't be skeptical about the number of participants ...

---

... for qualitative research identifies themes through small sample interviews.

Since security as a science is a elusive goal And its products are for human
use,

---

we are left wondering what makes a good paper?

---

Novelty was the only common metric, mentioned by 90% of participants, compared
with correctness with a 40% acknowledgment.

---

If evaluating is to subjective, rejecting is much more concrete and diverse.
Mess up any of these items and you'll be waved the red flag.

---

But it seems that with the diversity of subjects and mismatches with reviewers,
consistency is the price to pay when making room for novelty.

---

Overall strong accepts and rejects seem to be consistent, but for the middle
range, random. This leaves authors to resubmit to different venues until they
"get lucky" "gaming" the system.

---

---

In a broader perspective the traits of a good reviewer are clear, and Chairs
seem to understand best their role in shaping the program.

---

Really, it's the little things that make a good review stick out, like including
a paper summary. But wheres the drawback if we've already met the good reviewer
and review?

---

It appears that favoritism, imposed authority and authors "getting lucky" with
the next deadline drive the system to a point of malfunction.

---

So how does this relate to our original contextual problems?

---

We see that rolling submissions opens the possibility of dialogue, but rush
papers for publication, which only stresses reviewers.

---

We can't part from delegation as some senior members would leave the committee
otherwise.

---

So we come to the recommendation

----

to mentor or shadow the novice reviewers, who were formerly thrown a delegated
task. And this is actionable initiative which has already started.

Better time management could be done by simply asking them, to submit a paper
summaries early.

---

Rewarding good behavior will make them more motivated.

And last but not least, accountability will end the "gaming" of the system.

---

So, I leave you thinking about what writing a good review is and the bigger
picture. Thank you!
